# üöÄ Unified Cache System - Guia Completo

## üìã Vis√£o Geral

O **Unified Cache Manager** substitui 5+ implementa√ß√µes fragmentadas de cache no Darwin com um sistema unificado de alta performance.

### Benef√≠cios

‚úÖ **Performance**: 30-50% redu√ß√£o em lat√™ncia  
‚úÖ **Hit Rate**: Target >85% (vs. 60% anterior)  
‚úÖ **Consist√™ncia**: Uma √∫nica API para todo o codebase  
‚úÖ **Observabilidade**: M√©tricas detalhadas por layer  
‚úÖ **Flexibilidade**: 4 layers configur√°veis (L1-L4)

---

## üèóÔ∏è Arquitetura

### Cache Layers

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  L1: Memory (in-process)                ‚îÇ
‚îÇ  Latency: 100Œºs                         ‚îÇ
‚îÇ  Capacity: 512MB (configurable)         ‚îÇ
‚îÇ  Policy: LRU                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ fallback
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  L2: Redis (network)                    ‚îÇ
‚îÇ  Latency: 1-5ms                         ‚îÇ
‚îÇ  Capacity: Unlimited                    ‚îÇ
‚îÇ  Persistence: Yes (configurable TTL)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ fallback
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  L3: Semantic (Qdrant)                  ‚îÇ
‚îÇ  Latency: 10-50ms                       ‚îÇ
‚îÇ  Feature: Similarity search             ‚îÇ
‚îÇ  Threshold: 90% similarity              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ fallback
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  L4: Disk (persistent)                  ‚îÇ
‚îÇ  Latency: 100-500ms                     ‚îÇ
‚îÇ  Capacity: 2GB (configurable)           ‚îÇ
‚îÇ  Persistence: Yes                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Quick Start

### Basic Usage

```python
from darwin_core.app.services.unified_cache import get_unified_cache

# Get singleton instance
cache = get_unified_cache()
await cache.initialize()

# Set value
await cache.set("user_123", {"name": "Alice", "role": "admin"}, ttl=300)

# Get value (auto multi-layer fallback)
user = await cache.get("user_123")
print(user)  # {'name': 'Alice', 'role': 'admin'}

# Cache miss returns default
product = await cache.get("product_999", default=None)
print(product)  # None

# Get statistics
stats = cache.get_stats()
print(f"Hit rate: {stats.hit_rate:.1f}%")
print(f"L1 hits: {stats.l1_hits}")
print(f"L2 hits: {stats.l2_hits}")
```

### Using the @cached Decorator

```python
from darwin_core.app.services.unified_cache import cached

@cached(ttl=3600, key_prefix="expensive_query")
async def fetch_user_data(user_id: str):
    """Expensive database query - automatically cached"""
    # Expensive operation here
    result = await db.query(f"SELECT * FROM users WHERE id = {user_id}")
    return result

# First call: executes function and caches result
user = await fetch_user_data("123")

# Second call: returns cached result (no DB query)
user = await fetch_user_data("123")
```

---

## üìñ API Reference

### CacheConfig

Configuration for cache behavior:

```python
from darwin_core.app.services.unified_cache import CacheConfig

config = CacheConfig(
    # Memory cache (L1)
    max_memory_mb=512,
    memory_eviction_policy="lru",
    
    # Redis cache (L2)
    redis_host="localhost",
    redis_port=6379,
    redis_enabled=True,
    
    # Semantic cache (L3)
    semantic_enabled=True,
    semantic_similarity_threshold=0.90,
    
    # Disk cache (L4)
    disk_enabled=True,
    disk_cache_dir="/tmp/darwin_cache",
    max_disk_mb=2048,
    
    # TTL
    default_ttl_seconds=3600,
    
    # Compression
    enable_compression=True,
    compression_threshold_bytes=1024
)
```

### Core Methods

#### `async get(key, default=None)`

Get value from cache with multi-layer fallback.

```python
value = await cache.get("my_key", default="not_found")
```

**Behavior**:
1. Check L1 (Memory) - instant
2. If miss, check L2 (Redis) - 1-5ms
3. If miss, check L3 (Semantic) - 10-50ms
4. If miss, check L4 (Disk) - 100-500ms
5. Return default if all miss

**Promotion**: Lower layer hits automatically promoted to higher layers.

---

#### `async set(key, value, ttl=None, layers=None)`

Set value in cache.

```python
# Set in all available layers
await cache.set("user_data", user_obj, ttl=300)

# Set only in specific layers
from darwin_core.app.services.unified_cache import CacheLayer

await cache.set(
    "temp_data",
    data,
    ttl=60,
    layers=[CacheLayer.MEMORY]  # Only L1
)
```

**Parameters**:
- `key`: Cache key (string)
- `value`: Any picklable Python object
- `ttl`: Time to live in seconds (None = use default)
- `layers`: List of layers to write to (None = all available)

---

#### `async delete(key)`

Delete key from all layers.

```python
deleted = await cache.delete("user_123")
```

---

#### `async invalidate(pattern)`

Invalidate all keys matching pattern.

```python
# Invalidate all user cache entries
count = await cache.invalidate("user_*")
print(f"Invalidated {count} keys")

# Invalidate specific pattern
await cache.invalidate("session_2024*")
```

**Supports wildcards**: `*` matches any characters.

---

#### `async clear()`

Clear all cache layers.

```python
await cache.clear()
```

‚ö†Ô∏è  **Warning**: This deletes ALL cached data. Use with caution.

---

#### `get_stats()`

Get cache statistics.

```python
stats = cache.get_stats()

print(f"Total hits: {stats.hits}")
print(f"Total misses: {stats.misses}")
print(f"Hit rate: {stats.hit_rate:.1f}%")
print(f"L1 hits: {stats.l1_hits}")
print(f"L2 hits: {stats.l2_hits}")
print(f"L3 hits: {stats.l3_hits}")
print(f"L4 hits: {stats.l4_hits}")
print(f"Avg GET time: {stats.avg_get_time_ms:.2f}ms")
print(f"Evictions: {stats.evictions}")
```

---

## üîß Advanced Usage

### Custom Configuration per Service

```python
from darwin_core.app.services.unified_cache import UnifiedCacheManager, CacheConfig

# Custom config for specific service
config = CacheConfig(
    max_memory_mb=1024,  # 1GB memory
    redis_enabled=True,
    semantic_enabled=False,  # Disable semantic for this service
    default_ttl_seconds=7200  # 2 hours
)

service_cache = UnifiedCacheManager(config)
await service_cache.initialize()
```

---

### Conditional Caching

```python
async def get_data(key: str, force_refresh: bool = False):
    """Get data with optional cache bypass"""
    
    if not force_refresh:
        # Try cache first
        cached = await cache.get(key)
        if cached is not None:
            return cached
    
    # Fetch fresh data
    data = await expensive_fetch(key)
    
    # Cache for next time
    await cache.set(key, data, ttl=300)
    
    return data
```

---

### Batch Operations

```python
async def get_multiple(keys: List[str]) -> Dict[str, Any]:
    """Get multiple keys efficiently"""
    results = {}
    
    # Get all in parallel
    values = await asyncio.gather(*[
        cache.get(key) for key in keys
    ])
    
    for key, value in zip(keys, values):
        if value is not None:
            results[key] = value
    
    return results
```

---

### Cache Warming

```python
async def warm_cache():
    """Pre-populate cache with frequently accessed data"""
    
    # Warm user data
    users = await db.get_active_users()
    for user in users:
        await cache.set(f"user_{user.id}", user, ttl=3600)
    
    # Warm product catalog
    products = await db.get_popular_products(limit=100)
    for product in products:
        await cache.set(f"product_{product.id}", product, ttl=7200)
    
    logger.info(f"Cache warmed with {len(users)} users and {len(products)} products")
```

---

## üìä Monitoring & Observability

### Prometheus Metrics

Add to your Prometheus config:

```python
from prometheus_client import Gauge, Counter

# Cache metrics
cache_hit_rate = Gauge('darwin_cache_hit_rate_percent', 'Cache hit rate percentage')
cache_l1_hits = Counter('darwin_cache_l1_hits_total', 'L1 cache hits')
cache_l2_hits = Counter('darwin_cache_l2_hits_total', 'L2 cache hits')
cache_misses = Counter('darwin_cache_misses_total', 'Cache misses')
cache_evictions = Counter('darwin_cache_evictions_total', 'Cache evictions')

# Update periodically
async def update_cache_metrics():
    while True:
        stats = cache.get_stats()
        cache_hit_rate.set(stats.hit_rate)
        cache_l1_hits.inc(stats.l1_hits)
        cache_l2_hits.inc(stats.l2_hits)
        cache_misses.inc(stats.misses)
        cache_evictions.inc(stats.evictions)
        
        await asyncio.sleep(60)  # Update every minute
```

---

### Logging

```python
import logging

# Enable debug logging for cache
logging.getLogger('darwin_core.app.services.unified_cache').setLevel(logging.DEBUG)

# Logs will show:
# - Cache hits/misses
# - Layer promotions
# - Evictions
# - Errors
```

---

## üîÑ Migration Guide

### Migrating from Old Cache Implementations

**Before** (semantic_cache.py):
```python
from app.services.semantic_cache import SemanticCache

cache = SemanticCache()
await cache.initialize()
result = await cache.get("key")
```

**After** (unified_cache.py):
```python
from app.services.unified_cache import get_unified_cache

cache = get_unified_cache()
await cache.initialize()
result = await cache.get("key")
```

---

### Migrating Embedding Manager

**Before**:
```python
# In embedding_manager.py
self._cache: Dict[str, np.ndarray] = {}

def encode(self, texts):
    cache_key = self._hash(texts)
    if cache_key in self._cache:
        return self._cache[cache_key]
    
    embeddings = self._model.encode(texts)
    self._cache[cache_key] = embeddings
    return embeddings
```

**After**:
```python
from app.services.unified_cache import get_unified_cache

self._cache = get_unified_cache()

async def encode(self, texts):
    cache_key = self._hash(texts)
    
    # Try cache
    embeddings = await self._cache.get(cache_key)
    if embeddings is not None:
        return embeddings
    
    # Compute
    embeddings = self._model.encode(texts)
    
    # Cache (L1+L2 for fast access)
    await self._cache.set(cache_key, embeddings, ttl=3600)
    
    return embeddings
```

---

## üéØ Performance Tuning

### Memory Size

```python
# Small service (low memory)
CacheConfig(max_memory_mb=128)

# Medium service (normal)
CacheConfig(max_memory_mb=512)

# Large service (high memory available)
CacheConfig(max_memory_mb=2048)
```

---

### TTL Strategy

```python
# Frequently changing data - short TTL
await cache.set("stock_price", price, ttl=60)  # 1 minute

# Moderate change rate - medium TTL
await cache.set("user_profile", profile, ttl=3600)  # 1 hour

# Rarely changing - long TTL
await cache.set("product_catalog", catalog, ttl=86400)  # 24 hours

# Permanent (until explicit invalidation)
await cache.set("static_config", config, ttl=None)
```

---

### Layer Selection

```python
# Hot data - memory only (fastest)
await cache.set("session_token", token, layers=[CacheLayer.MEMORY])

# Important data - memory + redis (fast + persistent)
await cache.set("user_data", user, layers=[CacheLayer.MEMORY, CacheLayer.REDIS])

# Large data - redis + disk (persistent, space-efficient)
await cache.set("ml_model", model, layers=[CacheLayer.REDIS, CacheLayer.DISK])
```

---

## üêõ Troubleshooting

### Cache Hit Rate < 80%

**Causes**:
- TTL too short
- Keys too specific (low reusability)
- Working set > cache size

**Solutions**:
```python
# Increase TTL
config.default_ttl_seconds = 7200  # 2 hours

# Increase memory
config.max_memory_mb = 1024

# Use broader keys
# Bad:  "user_123_2024-10-28_14:30:45"
# Good: "user_123"
```

---

### High Memory Usage

**Causes**:
- Large values cached
- No eviction happening
- Memory leak

**Solutions**:
```python
# Enable compression
config.enable_compression = True

# Lower memory limit (force more evictions)
config.max_memory_mb = 256

# Use disk for large values
await cache.set("large_data", data, layers=[CacheLayer.DISK])

# Check for leaks
stats = cache.get_stats()
print(f"Memory entries: {len(cache._memory_cache)}")
print(f"Memory bytes: {cache._current_memory_bytes / 1024 / 1024:.1f}MB")
```

---

### Redis Connection Errors

```python
# Check Redis availability
if cache._redis_client:
    try:
        await cache._redis_client.ping()
        print("‚úÖ Redis connected")
    except:
        print("‚ùå Redis unavailable")

# Disable Redis if unavailable
config.redis_enabled = False
```

---

## üìà Performance Benchmarks

### Expected Performance

| Layer | Latency | Throughput |
|-------|---------|------------|
| L1 (Memory) | <100Œºs | >100k ops/s |
| L2 (Redis) | 1-5ms | >10k ops/s |
| L3 (Semantic) | 10-50ms | >100 ops/s |
| L4 (Disk) | 100-500ms | >50 ops/s |

### Hit Rate Targets

| Scenario | Target Hit Rate |
|----------|----------------|
| User sessions | >90% |
| API responses | >85% |
| Database queries | >80% |
| ML predictions | >75% |

---

## üîí Security Considerations

### Sensitive Data

```python
# Don't cache sensitive data in L2 (Redis) or L4 (Disk)
# if Redis/disk are shared or unsecured

await cache.set(
    "password_hash",
    hash_value,
    layers=[CacheLayer.MEMORY]  # Only in-process
)
```

### Access Control

```python
# Implement key prefixing by user/tenant
def get_user_cache_key(user_id: str, resource: str) -> str:
    return f"user_{user_id}:{resource}"

key = get_user_cache_key("123", "profile")
await cache.set(key, profile)
```

---

## üéì Best Practices

1. **Use meaningful key names**: `user_123_profile` not `abc123`
2. **Set appropriate TTLs**: Don't cache forever, but don't make it too short
3. **Monitor hit rates**: Target >80% for most use cases
4. **Invalidate proactively**: Clear cache when data changes
5. **Use decorators**: For simple function result caching
6. **Layer selection**: Hot data ‚Üí Memory, Important ‚Üí Redis, Large ‚Üí Disk
7. **Compression**: Enable for large values (>1KB)
8. **Batch operations**: Use `asyncio.gather()` for multiple gets
9. **Cache warming**: Pre-populate on startup for better initial performance
10. **Statistics tracking**: Monitor and optimize based on metrics

---

## üìû Support

**Questions?** Open an issue or contact the Darwin team.

**Documentation**: See `DARWIN_2025_DEEP_ANALYSIS_IMPROVEMENTS.md` for more context.

---

**Status**: ‚úÖ Production Ready  
**Version**: 1.0.0  
**Last Updated**: 2025-10-28

