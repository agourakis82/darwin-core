"""
vLLM Server with Hugging Face Models - REAL IMPLEMENTATION

High-performance LLM serving with:
- PagedAttention for memory efficiency
- Continuous batching
- GPU acceleration
- Streaming generation
"""

import logging
import torch
from typing import List, Dict, Any, Optional, Iterator
from dataclasses import dataclass

try:
    from vllm import LLM, SamplingParams
    HAS_VLLM = True
except ImportError:
    HAS_VLLM = False

logger = logging.getLogger(__name__)


@dataclass
class vLLMConfig:
    """Configuration for vLLM server"""
    model_name: str = "HuggingFaceTB/SmolLM2-360M-Instruct"
    tensor_parallel_size: int = 1  # Number of GPUs
    gpu_memory_utilization: float = 0.9
    max_model_len: int = 4096
    dtype: str = "auto"
    
    # Sampling defaults
    temperature: float = 0.7
    top_p: float = 0.9
    max_tokens: int = 512


class vLLMServer:
    """
    vLLM server for efficient LLM serving.
    
    Features:
    - PagedAttention (memory efficient)
    - Continuous batching (high throughput)
    - Multi-GPU support
    - Fast inference
    """
    
    def __init__(self, config: Optional[vLLMConfig] = None):
        if not HAS_VLLM:
            raise ImportError("vllm required: pip install vllm")
        
        self.config = config or vLLMConfig()
        self.llm = None
        
        logger.info(f"vLLM Server initializing with {self.config.model_name}")
        self._load_model()
    
    def _load_model(self):
        """Load model with vLLM"""
        try:
            self.llm = LLM(
                model=self.config.model_name,
                tensor_parallel_size=self.config.tensor_parallel_size,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                max_model_len=self.config.max_model_len,
                dtype=self.config.dtype,
                trust_remote_code=True
            )
            logger.info("vLLM model loaded successfully!")
        except Exception as e:
            logger.error(f"Failed to load vLLM model: {e}")
            raise
    
    def generate(
        self,
        prompts: List[str],
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stop: Optional[List[str]] = None
    ) -> List[str]:
        """
        Generate completions for prompts.
        
        Args:
            prompts: List of prompts
            temperature: Sampling temperature
            top_p: Nucleus sampling
            max_tokens: Max tokens to generate
            stop: Stop sequences
        
        Returns:
            List of generated texts
        """
        if not self.llm:
            raise RuntimeError("Model not loaded")
        
        sampling_params = SamplingParams(
            temperature=temperature or self.config.temperature,
            top_p=top_p or self.config.top_p,
            max_tokens=max_tokens or self.config.max_tokens,
            stop=stop
        )
        
        outputs = self.llm.generate(prompts, sampling_params)
        
        # Extract generated texts
        results = []
        for output in outputs:
            text = output.outputs[0].text
            results.append(text)
        
        logger.info(f"Generated {len(results)} completions")
        return results
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> str:
        """
        Chat completion (single conversation).
        
        Args:
            messages: List of {"role": "user/assistant", "content": "..."}
            temperature: Sampling temperature
            max_tokens: Max tokens
        
        Returns:
            Assistant response
        """
        # Format as chat prompt
        prompt = self._format_chat(messages)
        
        results = self.generate(
            [prompt],
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        return results[0] if results else ""
    
    def _format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages as chat prompt"""
        parts = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                parts.append(f"<|im_start|>system\n{content}<|im_end|>")
            elif role == "user":
                parts.append(f"<|im_start|>user\n{content}<|im_end|>")
            elif role == "assistant":
                parts.append(f"<|im_start|>assistant\n{content}<|im_end|>")
        
        # Add assistant start
        parts.append("<|im_start|>assistant\n")
        
        return "\n".join(parts)
    
    def get_stats(self) -> Dict[str, Any]:
        """Get server statistics"""
        return {
            "model": self.config.model_name,
            "gpus": self.config.tensor_parallel_size,
            "max_model_len": self.config.max_model_len,
            "gpu_memory_util": self.config.gpu_memory_utilization
        }


# Test
if __name__ == "__main__":
    import sys
    import time
    
    print("="*70)
    print("vLLM Server with Hugging Face Models - REAL TEST")
    print("="*70)
    
    # Check GPU
    if torch.cuda.is_available():
        print(f"\n‚úÖ GPU Available: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("\n‚ö†Ô∏è  No GPU, using CPU (not recommended for vLLM)")
    
    # Create vLLM server
    print("\nInitializing vLLM server...")
    config = vLLMConfig(
        model_name="HuggingFaceTB/SmolLM2-360M-Instruct",
        gpu_memory_utilization=0.5,  # Conservative for testing
        max_model_len=2048
    )
    
    server = vLLMServer(config)
    
    print("\nüìä Server stats:")
    stats = server.get_stats()
    for k, v in stats.items():
        print(f"  {k}: {v}")
    
    # Test 1: Single generation
    print("\n" + "="*70)
    print("TEST 1: Single generation")
    prompts = ["What is the capital of France?"]
    
    start = time.time()
    results = server.generate(prompts, max_tokens=50)
    elapsed = time.time() - start
    
    print(f"Prompt: {prompts[0]}")
    print(f"Response: {results[0]}")
    print(f"‚è±Ô∏è  Time: {elapsed:.2f}s")
    
    # Test 2: Batch generation
    print("\n" + "="*70)
    print("TEST 2: Batch generation (continuous batching)")
    prompts = [
        "What is PCL?",
        "Define scaffold porosity.",
        "Explain 3D printing."
    ]
    
    start = time.time()
    results = server.generate(prompts, max_tokens=50)
    elapsed = time.time() - start
    
    for i, (prompt, result) in enumerate(zip(prompts, results)):
        print(f"\n{i+1}. {prompt}")
        print(f"   ‚Üí {result[:100]}...")
    
    print(f"\n‚è±Ô∏è  Total time: {elapsed:.2f}s ({elapsed/len(prompts):.2f}s per prompt)")
    print(f"üöÄ Throughput: {len(prompts)/elapsed:.2f} prompts/sec")
    
    # Test 3: Chat
    print("\n" + "="*70)
    print("TEST 3: Chat completion")
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What are scaffolds used for in tissue engineering?"}
    ]
    
    start = time.time()
    response = server.chat(messages, max_tokens=100)
    elapsed = time.time() - start
    
    print(f"User: {messages[1]['content']}")
    print(f"Assistant: {response}")
    print(f"‚è±Ô∏è  Time: {elapsed:.2f}s")
    
    print("\n" + "="*70)
    print("‚úÖ vLLM server with PagedAttention works!")
    sys.exit(0)

